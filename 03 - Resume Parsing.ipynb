{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "df_resume = pd.read_csv(\"data/Resume.csv\")\n",
    "df_resume.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.Category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume = df_resume.reindex(np.random.permutation(df_resume.index))\n",
    "df_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume = df_resume.copy().iloc[:1000, ]\n",
    "df_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load skill data\n",
    "\n",
    "If we define patterns for all the skill, we gonna be too tired.\n",
    "\n",
    "So spacy knows that, so it allows you to give you a list of words, then it will automatically create pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koala/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component         Assigns               Requires   Scores             Retokenizes\n",
      "-   ---------------   -------------------   --------   ----------------   -----------\n",
      "0   tok2vec           doc.tensor                                          False      \n",
      "                                                                                     \n",
      "1   tagger            token.tag                        tag_acc            False      \n",
      "                                                                                     \n",
      "2   parser            token.dep                        dep_uas            False      \n",
      "                      token.head                       dep_las                       \n",
      "                      token.is_sent_start              dep_las_per_type              \n",
      "                      doc.sents                        sents_p                       \n",
      "                                                       sents_r                       \n",
      "                                                       sents_f                       \n",
      "                                                                                     \n",
      "3   attribute_ruler                                                       False      \n",
      "                                                                                     \n",
      "4   lemmatizer        token.lemma                      lemma_acc          False      \n",
      "                                                                                     \n",
      "5   ner               doc.ents                         ents_f             False      \n",
      "                      token.ent_iob                    ents_p                        \n",
      "                      token.ent_type                   ents_r                        \n",
      "                                                       ents_per_type                 \n",
      "\n",
      "\u001b[38;5;2mâœ” No problems found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp        = spacy.load('en_core_web_md')\n",
    "analysis = nlp.analyze_pipes(pretty=True)\n",
    "skill_path = './app/data/jz_skill_patterns.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner',\n",
       " 'entity_ruler']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.from_disk(skill_path) # lodad ruler from outside\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Chaky loves ajax.\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Let's try to extract skills from this resume.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean our data\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def preprocessing(sentence):\n",
    "\n",
    "    stopwords    = list(STOP_WORDS)\n",
    "    doc          = nlp(sentence)\n",
    "    clean_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text not in stopwords and token.pos_ != 'PUNCT' and token.pos_ != 'SYM' and \\\n",
    "            token.pos_ != 'SPACE':\n",
    "                clean_tokens.append(token.lemma_.lower().strip())\n",
    "                \n",
    "    return \" \".join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_resume = df_resume.Resume_str.iloc[5]\n",
    "random_resume[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(random_resume[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df_resume.iterrows():\n",
    "    clean_text = preprocessing(row.Resume_str)\n",
    "    df_resume.at[i, 'Clean_resume'] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's really extract skills!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_skills(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    skills = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if re.match(r'^SKILL', ent.label_):\n",
    "            skills.append(ent.text)\n",
    "            \n",
    "    return skills\n",
    "\n",
    "def unique_skills(x):\n",
    "    return list(set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume = df_resume[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume['Skills'] = df_resume.Clean_resume.apply(get_skills)\n",
    "df_resume['Skills'] = df_resume.Skills.apply(unique_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.Skills.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume.Skills.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_pattern = [{'label': 'EMAIL', \n",
    "                  'pattern': [{'TEXT': {'REGEX': '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'}}]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(email_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"st124092@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "\n",
    "Which skills is most important in information management?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_resume.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'INFORMATION-TECHNOLOGY'\n",
    "cond     = df_resume.Category == category\n",
    "\n",
    "df_resume_it = df_resume[cond]\n",
    "df_resume_it.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_skills = np.concatenate(df_resume_it.Skills.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter, OrderedDict\n",
    "\n",
    "# counting = Counter(all_skills)\n",
    "# counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting = OrderedDict(counting.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(15, 3))\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "# plt.bar(counting.keys(), counting.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Name Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define patterns\n",
    "patterns = [\n",
    "    {\"label\": \"EDUCATION\", \"pattern\": [{\"LOWER\": {\"IN\": [\"bsc\", \"bachelor\", \"bachelor's\", \"b.a\", \"b.s\"]}}, {\"IS_ALPHA\": True, \"OP\": \"*\"}]},\n",
    "    {\"label\": \"EDUCATION\", \"pattern\": [{\"LOWER\": {\"IN\": [\"msc\", \"master\", \"master's\", \"m.a\", \"m.s\"]}}, {\"IS_ALPHA\": True, \"OP\": \"*\"}]},\n",
    "    {\"label\": \"EDUCATION\", \"pattern\": [{\"LOWER\": {\"IN\": [\"phd\", \"ph.d\", \"doctor\", \"doctorate\"]}}, {\"IS_ALPHA\": True, \"OP\": \"*\"}]}\n",
    "]\n",
    "\n",
    "# Add patterns to the entity ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Add the entity ruler to the pipeline\n",
    "# nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Test text\n",
    "text = \"I completed my bachelor in engineering\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_pattern = [\n",
    "    # Patterns for COMPANY to capture more context around keywords like \"university\", \"company\", etc.\n",
    "    {\"label\": \"COMPANY\", \"pattern\": [{\"LOWER\": {\"IN\": [\"university\", \"institute\", \"college\", \"school\"]}}, {\"IS_ALPHA\": True, \"OP\": \"+\"}]},\n",
    "    {\"label\": \"COMPANY\", \"pattern\": [\n",
    "        {\"IS_ALPHA\": True, \"OP\": \"*\"},  # Zero or more tokens before the keyword\n",
    "        {\"LOWER\": {\"IN\": [\"company\", \"corporation\", \"inc\", \"ltd\"]}},  # Match keywords in lowercase\n",
    "        {\"IS_ALPHA\": True, \"OP\": \"?\"},  # Optionally match a word after the keyword\n",
    "    ]}\n",
    "]\n",
    "\n",
    "# Assuming you have already initialized your nlp object and the EntityRuler as shown previously\n",
    "ruler.add_patterns(company_pattern)\n",
    "\n",
    "# Assuming the EntityRuler has been added to the pipeline\n",
    "text = \"I worked at jmm core corporation\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# education_patterns = [\n",
    "#     {\n",
    "#         \"label\": \"EDUCATION\",\n",
    "#         \"pattern\": [\n",
    "#             {\"LOWER\": {\"IN\": [\"b.sc\", \"m.sc\", \"bachelor\", \"master\", \"doctoral\", \"post-doctoral\", \"b.a\", \"m.a\", \"b.com\", \"m.com\", \"ph.d\", \"bsc\", \"msc\", \"ba\", \"ma\", \"bcom\", \"mcom\", \"phd\"]}}\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"label\": \"EDUCATION\",\n",
    "#         \"pattern\": [\n",
    "#             {\"LOWER\": \"bachelor\", \"OP\": \"?\"},\n",
    "#             {\"LOWER\": \"of\", \"OP\": \"?\"},\n",
    "#             {\"POS\": \"NOUN\", \"OP\": \"+\"}\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"label\": \"EDUCATION\",\n",
    "#         \"pattern\": [\n",
    "#             {\"LOWER\": \"master\", \"OP\": \"?\"},\n",
    "#             {\"LOWER\": \"of\", \"OP\": \"?\"},\n",
    "#             {\"POS\": \"NOUN\", \"OP\": \"+\"}\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"label\": \"EDUCATION\",\n",
    "#         \"pattern\": [\n",
    "#             {\"LOWER\": {\"IN\": [\"phd\", \"ph.d\", \"d.phil\"]}},\n",
    "#             {\"LOWER\": \"in\", \"OP\": \"?\"},\n",
    "#             {\"POS\": \"PROPN\", \"OP\": \"+\"}\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"label\": \"EDUCATION\",\n",
    "#         \"pattern\": [\n",
    "#             {\"LOWER\": {\"IN\": [\"doctor\", \"dr\"]}},\n",
    "#             {\"IS_PUNCT\": True, \"OP\": \"?\"},\n",
    "#             {\"POS\": \"PROPN\", \"OP\": \"+\"}\n",
    "#         ]\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# ruler.add_patterns(education_patterns)\n",
    "\n",
    "# example_text = \"I am graduate from university of technology ycc with bachelor of Engineering (Mechanical Precision and Automation)\"\n",
    "# # Process the text through the pipeline\n",
    "# doc = nlp(example_text)\n",
    "\n",
    "# # Display the entities\n",
    "# for ent in doc.ents:\n",
    "#     print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Website_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_patterns = [\n",
    "    {\"label\": \"WEBSITE\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"}}]}\n",
    "]\n",
    "\n",
    "# Add patterns to the ruler\n",
    "ruler.add_patterns(web_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"If you want to visit the repository, go to http://github.com/kaunghtetcho until you want to stop.\"\n",
    "# Process the text through the pipeline\n",
    "doc = nlp(example_text)\n",
    "\n",
    "# Display the entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pattern = [\n",
    "    {\"label\": \"DATE\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \n",
    "                          \"august\", \"september\", \"october\", \"november\", \"december\"]}},\n",
    "        {\"SHAPE\": \"dddd\"},\n",
    "        {\"LOWER\": \"-\", \"OP\": \"?\"},\n",
    "        {\"LOWER\": {\"IN\": [\"-\", \"â€“\"]}, \"OP\": \"?\"},\n",
    "        {\"LOWER\": {\"IN\": [\"present\", \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \n",
    "                          \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]}, \"OP\": \"?\"},\n",
    "        {\"SHAPE\": \"dddd\", \"OP\": \"?\"}\n",
    "    ]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(date_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"JAN 2019 - JAN 2024\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text through the pipeline\n",
    "doc = nlp(example_text)\n",
    "\n",
    "# Display the entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_pattern = [\n",
    "# {\n",
    "#     \"label\": \"JOB_TITLE\",\n",
    "#     \"pattern\": [\n",
    "#         {\"POS\": \"NOUN\", \"OP\": \"+\"},  # One or more nouns in sequence\n",
    "#         {\"POS\": \"ADP\", \"OP\": \"?\"},   # An optional preposition\n",
    "#         {\"POS\": \"NOUN\", \"OP\": \"+\"}   # Another sequence of one or more nouns\n",
    "#     ]\n",
    "# },\n",
    "# {\n",
    "#     \"label\": \"JOB_TITLE\",\n",
    "#     \"pattern\": [\n",
    "#         {\"POS\": \"PROPN\", \"OP\": \"+\"},  # Sequence of proper nouns\n",
    "#         {\"POS\": \"ADP\", \"OP\": \"?\"},    # An optional preposition\n",
    "#         {\"POS\": \"PROPN\", \"OP\": \"?\"}   # Optional proper noun\n",
    "#     ]\n",
    "# },\n",
    "# {\n",
    "#     \"label\": \"JOB_TITLE\",\n",
    "#     \"pattern\": [\n",
    "#         {\"TEXT\": {\"REGEX\": \"^[A-Z][a-z]+\"}},  # Starts with a capital letter followed by lowercase\n",
    "#         {\"IS_PUNCT\": True, \"OP\": \"?\"},         # Optional punctuation\n",
    "#         {\"TEXT\": {\"REGEX\": \"^[A-Z][a-z]+\"}, \"OP\": \"*\"}  # Zero or more additional words with the same pattern\n",
    "#     ]\n",
    "# }\n",
    "# ]\n",
    "\n",
    "# ruler.add_patterns(job_pattern)\n",
    "\n",
    "# # Test the pipeline\n",
    "# doc = nlp(\"I worked as an Engineer in Myanmar for three years\")\n",
    "\n",
    "# for ent in doc.ents:\n",
    "#     print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Let's load the PDF - add some realism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(\"data/chaklam_resume.pdf\")\n",
    "page   = reader.pages[0]\n",
    "text   = page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocessing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "colors = {\"SKILL\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\"}\n",
    "colors[\"EDUCATION\"] = \"linear-gradient(90deg, #ffd700, #ff6347)\" \n",
    "colors[\"EMAIL\"] = \"linear-gradient(90deg, #98fb98, #008000)\" \n",
    "colors[\"WEBSITE\"] = \"linear-gradient(90deg, #ffff00, #ffdd00)\"  # Example gradient from bright yellow to deep yellow\n",
    "colors[\"COMPANY\"] = \"linear-gradient(90deg, #ggg999, #jj1234)\" \n",
    "options = {\"colors\": colors}\n",
    "\n",
    "displacy.render(doc, style='ent', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('310 - 9191', 'QUANTITY')]\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {\n",
    "        \"label\": \"MOBILE\",\n",
    "        \"pattern\": [\n",
    "            {\"TEXT\": {\"REGEX\": \"\\\\+?\\\\d{1,3}\"}},  # Matches the country code, optional\n",
    "            {\"IS_SPACE\": True, \"OP\": \"?\"},  # Optional space\n",
    "            {\"TEXT\": \"-\", \"OP\": \"?\"},  # Optional dash\n",
    "            {\"IS_SPACE\": True, \"OP\": \"?\"},  # Optional space\n",
    "            {\"TEXT\": {\"REGEX\": \"\\\\d{1,4}\"}},  # Matches the first segment of the number\n",
    "            {\"IS_SPACE\": True, \"OP\": \"?\"},  # Optional space\n",
    "            {\"TEXT\": \"-\", \"OP\": \"?\"},  # Optional dash\n",
    "            {\"IS_SPACE\": True, \"OP\": \"?\"},  # Optional space\n",
    "            {\"TEXT\": {\"REGEX\": \"\\\\d{2,3}\"}},  # Matches the second segment of the number\n",
    "            {\"IS_SPACE\": True, \"OP\": \"?\"},  # Optional space\n",
    "            {\"TEXT\": \"-\", \"OP\": \"?\"},  # Optional dash\n",
    "            {\"IS_SPACE\": True, \"OP\": \"?\"},  # Optional space\n",
    "            {\"TEXT\": {\"REGEX\": \"\\\\d{3}\"}},  # Matches the third segment of the number\n",
    "            {\"IS_SPACE\": True, \"OP\": \"?\"},  # Optional space\n",
    "            {\"TEXT\": \"-\", \"OP\": \"?\"},  # Optional dash\n",
    "            {\"IS_SPACE\": True, \"OP\": \"?\"},  # Optional space\n",
    "            {\"TEXT\": {\"REGEX\": \"\\\\d{4}\"}},  # Matches the fourth segment of the number\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "ruler.add_patterns(pattern)\n",
    "# nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp(\"my phone number is +66 - 63 310 - 9191\")\n",
    "\n",
    "# Example for demonstration\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_tfr'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_tfr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m ruler \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity_ruler\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_tfr'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_tfr')\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('+660824578605', 'MOBILE')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "patterns = [\n",
    "    {\"label\": \"MOBILE\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"\\\\+?\\\\d{1,3}[-. (]*\\\\d{3}[-. )]*\\\\d{3}[-. ]*\\\\d{4}(?: *x\\\\d+)?\\\\s*\"}}]}\n",
    "]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "# nlp.add_pipe(ruler)\n",
    "\n",
    "# Example usage\n",
    "text = \"my mobile number is master +660824578605\"\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])  # For demonstration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {\n",
    "        \"label\": \"MOBILE\",\n",
    "        \"pattern\": [{\"TEXT\": {\"REGEX\": \"\\\\+\\\\d{1,3}-\\\\d{2,3}-\\\\d{3}-\\\\d{4}\"}}]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add the pattern to the ruler and the ruler to the nlp pipeline\n",
    "ruler.add_patterns(pattern)\n",
    "# nlp.add_pipe('entity_ruler')\n",
    "\n",
    "# Test the pattern with an example text\n",
    "text = \"Call me at +66-63-310-9191.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print out the matched entities\n",
    "matches = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: NAME_INTRO\n",
      "John Doe PERSON\n"
     ]
    }
   ],
   "source": [
    "patterns = [\n",
    "    {\"label\": \"NAME_INTRO\", \"pattern\": [{\"LOWER\": \"name\"}, {\"IS_PUNCT\": True, \"OP\": \"?\"}, {\"LOWER\": \":\"}]},\n",
    "    {\"label\": \"NAME_INTRO\", \"pattern\": [{\"LOWER\": \"full\"}, {\"LOWER\": \"name\"}, {\"IS_PUNCT\": True, \"OP\": \"?\"}, {\"LOWER\": \":\"}]},\n",
    "    {\"label\": \"NAME_INTRO\", \"pattern\": [{\"LOWER\": \"first\"}, {\"LOWER\": \"name\"}, {\"IS_PUNCT\": True, \"OP\": \"?\"}, {\"LOWER\": \":\"}]},\n",
    "    # Add more patterns here as needed\n",
    "]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "# nlp.add_pipe(ruler)\n",
    "\n",
    "# Example text\n",
    "text = \"Name: John Doe\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Printing out the matches\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
